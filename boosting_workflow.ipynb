{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8f284b-9c76-4de4-a514-b2333a27287b",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4244e7-72ef-4ac7-88dd-b0382ddc930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507e4ef-b343-4c79-9ba1-9960ee3fd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing for Numerical Features\n",
    "def numerical_pipeline(strategy='mean'):\n",
    "    \"\"\"\n",
    "    Create a pipeline for numerical features: imputation followed by scaling.\n",
    "    \"\"\"\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=strategy)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    return num_pipeline\n",
    "\n",
    "# Data Pre-processing for Categorical Features\n",
    "def categorical_pipeline(strategy='most_frequent', encoding='onehot'):\n",
    "    \"\"\"\n",
    "    Create a pipeline for categorical features: imputation followed by encoding.\n",
    "    \"\"\"\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=strategy)),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore') if encoding == 'onehot' else None)\n",
    "    ])\n",
    "    return cat_pipeline\n",
    "\n",
    "# Full Pre-processing Pipeline combining both Numerical and Categorical\n",
    "def full_preprocessor(num_features, cat_features, num_strategy='mean', cat_strategy='most_frequent', cat_encoding='onehot'):\n",
    "    \"\"\"\n",
    "    Create a full pre-processing pipeline combining both numerical and categorical pipelines.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numerical_pipeline(num_strategy), num_features),\n",
    "        ('cat', categorical_pipeline(cat_strategy, cat_encoding), cat_features)\n",
    "    ])\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e1cd1-7ca7-48ea-8de5-2887be343ddc",
   "metadata": {},
   "source": [
    "## 2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e3ebe-5f53-4f28-990c-62499cc765f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, adjusted_r2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1294bae-49f6-4055-999c-604cc8371d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Functions for Regression including adjusted r2. \n",
    "def regression_metrics(y_true, y_pred, n_features):\n",
    "    \"\"\"\n",
    "    Calculate and return the commonly used regression metrics including MSE, RMSE, MAE, R2, and Adjusted R2 score.\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate Adjusted R2\n",
    "    adj_r2 = 1 - ((1 - r2) * (n_samples - 1) / (n_samples - n_features - 1))\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Adjusted R2': adj_r2}\n",
    "\n",
    "# Metric Functions for Classification\n",
    "def classification_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"\n",
    "    Calculate and return the commonly used classification metrics including Accuracy, Precision, Recall, F1, and AUC.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    auc = roc_auc_score(y_true, y_prob, multi_class='ovr', average='weighted') if y_prob is not None else 'NA'\n",
    "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1, 'AUC': auc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efdfd3-4705-4cf4-8622-e9b8cee81d78",
   "metadata": {},
   "source": [
    "## 3. Training and Performance Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87994f85-5db3-4c34-a416-c6c32505b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a4bec1-54d8-4c17-8ea3-71d0496f4f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble Block, ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "def ensemble_models(models: list, voting='soft', task='classification'):\n",
    "    \"\"\"\n",
    "    Create and return an ensemble model using the provided base models.\n",
    "    \"\"\"\n",
    "    if task == 'classification':\n",
    "        return VotingClassifier(estimators=models, voting=voting)\n",
    "    else:\n",
    "        return VotingRegressor(estimators=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a21aab-1e51-4938-a36f-b16e0b2070f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgb\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.05],  # Typical values for learning rate\n",
    "    'max_depth': [4,5,6],          # Maximum depth of the individual estimators\n",
    "    'min_child_weight': [1],       # Minimum sum of instance weight needed in a child\n",
    "    'subsample': [1.0],        # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.6], # Subsample ratio of columns when constructing each tree\n",
    "    'n_estimators': [100,200],     # Number of boosting rounds to be run\n",
    "#    'gamma': [0.1, 0.2],              # Minimum loss reduction required to make a further partition\n",
    "#    'scale_pos_weight': [1, 3],       # Control the balance of positive and negative weights\n",
    "#    'reg_alpha': [0, 0.1],          # L1 regularization term on weights\n",
    "#    'reg_lambda': [1, 2]            # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# LightGBM\n",
    "param_grid_lgb = {\n",
    "    'learning_rate': [0.01, 0.05],  # Learning rate\n",
    "    'num_leaves': [31, 63],         # Maximum tree leaves for base learners\n",
    "    'max_depth': [4, 5, 6],         # Maximum tree depth for base learners\n",
    "    'n_estimators': [100, 200],     # Number of boosting rounds\n",
    "    'min_split_gain': [0.0, 0.1],   # Minimum loss gain to perform a split\n",
    "    'min_child_weight': [1e-3, 1],  # Minimum sum of instance weight (hessian) needed in a child (leaf)\n",
    "    'subsample': [1.0, 0.8],        # Subsample ratio of the training instance\n",
    "    'colsample_bytree': [0.6, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "}\n",
    "# CatBoost\n",
    "param_grid_cat = {\n",
    "    'learning_rate': [0.01, 0.05],  # Learning rate\n",
    "    'depth': [4, 6, 8],             # Depth of the trees\n",
    "    'iterations': [100, 200],       # Number of boosting rounds\n",
    "#    'l2_leaf_reg': [1, 3],          # L2 regularization term on weights\n",
    "#    'border_count': [32, 64],       # Number of splits for numerical features\n",
    "#    'bagging_temperature': [0, 1]   # Controls the intensity of Bayesian bagging\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e3386-b611-4f18-a36d-d01c41891a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Workflows without embedding the models inside the function\n",
    "\n",
    "# Initialize Models\n",
    "xgb_model = XGBRegressor() # Or XGBClassifier\n",
    "cat_model = LGBMRegressor() # Or LGBMClassifier\n",
    "lgb_model =  CatBoostRegressor() # Or CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfc206-41e6-44df-aba9-7213cc668698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "X, y = DUMMY_DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a036e-1685-42a0-a9be-3f8d265a6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "grid_search_cat = GridSearchCV(cat_model, param_grid_cat, cv=5)\n",
    "grid_search_cat.fit(X_train, y_train)\n",
    "\n",
    "grid_search_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=5)\n",
    "grid_search_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1c6fb-58cc-4c95-93d3-86b0b19f742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters for each model\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_params_cat = grid_search_cat.best_params_\n",
    "best_params_lgb = grid_search_lgb.best_params_\n",
    "\n",
    "# Set the best parameters to each model\n",
    "xgb_model.set_params(**best_params_xgb)\n",
    "cat_model.set_params(**best_params_cat)\n",
    "lgb_model.set_params(**best_params_lgb)\n",
    "\n",
    "# Fit models with the entire training set after hyperparameter tuning\n",
    "xgb_model.fit(X_train, y_train)\n",
    "cat_model.fit(X_train, y_train)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Create ensemble model\n",
    "\n",
    "\n",
    "# Create ensemble model using the provided function\n",
    "ensemble_model = ensemble_models(\n",
    "    models=[('xgb', xgb_model),('lgb', lgb_model), ('cat', cat_model)],\n",
    "    task='regression'\n",
    ")\n",
    "\n",
    "# Fit ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e192a6f-0f7e-4415-8f8a-edafa4b8750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# Evaluate models using the metric functions\n",
    "metrics_xgb = regression_metrics(y_test, y_pred_xgb, n_features=X_train.shape[1])\n",
    "metrics_lgb = regression_metrics(y_test, y_pred_cat, n_features=X_train.shape[1])\n",
    "metrics_cat = regression_metrics(y_test, y_pred_cat, n_features=X_train.shape[1])\n",
    "metrics_ensemble = regression_metrics(y_test, y_pred_ensemble, n_features=X_train.shape[1])\n",
    "\n",
    "metrics_xgb, metrics_lgb,metrics_cat, metrics_ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
